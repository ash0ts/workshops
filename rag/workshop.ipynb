{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: From Simple to Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/parambharat/workshops/blob/main/rag/workshop.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Ensure you have a W&B account and API key. You should be able to get one from [here](https://wandb.ai/authorize).\n",
    "\n",
    "Next head over to [Tavily](https://app.tavily.com/home) and sign up for a free account and get your API key.\n",
    "\n",
    "We will use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) to install the workshop package and its dependencies.\n",
    "\n",
    "\n",
    "```bash\n",
    "!git clone https://github.com/parambharat/workshops.git\n",
    "%cd workshops/rag\n",
    "!pip install uv\n",
    "!uv sync\n",
    "```\n",
    "\n",
    "Create an `.env` file in the workshop/rag directory and add the API keys to it:\n",
    "\n",
    "```.env\n",
    "PINECONE_API_KEY=\"YOUR_PINECONE_API_KEY\"\n",
    "OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n",
    "TAVILY_API_KEY=\"YOUR_TAVILY_API_KEY\"\n",
    "WANDB_PROJECT=\"rag-workshop-iiit-blr\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/parambharat/workshops.git\n",
    "%cd workshops/rag\n",
    "!pip install -qqq uv \n",
    "!uv sync -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title API KEYS\n",
    "import os\n",
    "PINECONE_API_KEY = 'PINECONE_API_KEY' # @param {type:\"string\"}\n",
    "OPENAI_API_KEY = 'OPENAI_API_KEY' # @param {type:\"string\"}\n",
    "TAVILY_API_KEY = 'TAVILY_API_KEY' # @param {type:\"string\"}\n",
    "WANDB_PROJECT = \"rag-workshop-pc-nyc\" # @param {type:\"string\"}\n",
    "\n",
    "with open(\"./.env\", \"w+\") as env_f:\n",
    "  env_f.write(f'PINECONE_API_KEY=\"{PINECONE_API_KEY}\"\\n')\n",
    "  env_f.write(f'OPENAI_API_KEY=\"{OPENAI_API_KEY}\"\\n')\n",
    "  env_f.write(f'TAVILY_API_KEY=\"{TAVILY_API_KEY}\"\\n')\n",
    "  env_f.write(f'WANDB_PROJECT=\"{WANDB_PROJECT}\"')\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext dotenv\n",
    "# %dotenv\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "import weave\n",
    "from copy import deepcopy\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_contents_to_text, make_id, render_doc, printmd, chunk_simple, chunk_markdown, load_dataset, chunk_dataset, run_llm\n",
    "from retrieval_metrics import RetrievalScorer\n",
    "from response_metrics import ResponseScorer\n",
    "from retriever import TfidfSearchEngine, BM25SearchEngine, DenseSearchEngine, Retriever, RetrieverWithReranker, HybridRetrieverWithReranker, VectorStoreSearchEngine\n",
    "from generation import SimpleResponseGenerator, QueryEnhancedResponseGenerator\n",
    "from pipeline import SimpleRAGPipeline, QueryEnhancedRAGPipeline\n",
    "from query_enhancer import QueryEnhancer\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await run_llm(messages=[{\"role\": \"user\", \"content\": \"What is Weights and Biases and what is Pinecone?\"}])\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weave\n",
    "\n",
    "Weave is a lightweight toolkit for tracking and evaluating LLM applications ( This is a very simplified description of Weave though).\n",
    "\n",
    "- Log and debug language model inputs, outputs, and traces\n",
    "- Build rigorous, apples-to-apples evaluations for language model use cases\n",
    "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production\n",
    "\n",
    "Don't worry about the details for now. We will see more of it as we build our RAG pipeline.\n",
    "\n",
    "For now let's initialize W&B Weave. Once intialized, weave will start tracking (more on it later) the inputs and the outputs along with underlying attributes (model name, top_k, etc.) of all the LLMs, functions, evaluations, etc. we will be calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weave with the project name\n",
    "weave_client = weave.init(WANDB_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "We'll use [Weave Datasets](https://weave-docs.wandb.ai/guides/core-types/datasets) to track and version data as the inputs and outputs of our W&B Runs. \n",
    "\n",
    "To learn more about how we prepared the dataset for this workshop, checkout the [dataset preparation script](./download_finance_docs.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_finance_docs import PDFProcessor\n",
    "processor = PDFProcessor()\n",
    "data = processor.load_pdf_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = \"../data/finance_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = pathlib.Path(docs_dir)\n",
    "docs_files = sorted(docs_dir.rglob(\"*.pdf\"))\n",
    "\n",
    "print(f\"Number of files: {len(docs_files)}\\n\")\n",
    "print(\"First 5 files:\\n{files}\".format(files=\"\\n\".join(map(str, docs_files[:5]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our documents are stored as dictionaries with content (_raw text_) and additional metadata.\n",
    "\n",
    "Metadata is extra information for that data point which can be used to group together similar data points, or filter out a few data points.\n",
    "We will see in future chapters the importance of metadata and why it should not be ignored while building the ingestion pipeline.\n",
    "\n",
    "The metadata can be derived (`file_type`) or is inherent (`uri`) to the data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Words: \", sum(map(lambda x: len(x.split()), map(lambda x: x[\"content\"], docs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Checking the total number of tokens of your data source is a good practice. In this case, the total tokens is 500k+. Surely, most LLM providers cannot process this many tokens. Building a RAG is justified in such cases.\n",
    "\n",
    "**Note** We are simply counting words  and calling them `tokens` here. It's only an approximation of the actual token count which will be a bigger number.\n",
    "In practice we would be using a [tokenizer](https://docs.cohere.com/docs/tokens-and-tokenizers) to calculate the token counts but this naive calculation is an okay approximation for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build weave dataset\n",
    "raw_data = weave.Dataset(name=\"raw_data\", rows=data)\n",
    "\n",
    "# publish the dataset\n",
    "weave.publish(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple RAG pipeline\n",
    "\n",
    "First, we will build a very simple RAG pipeline.\n",
    "We will mainly focus on how to preprocess and chunk the data followed by building the simplest retrieval engine without using any fancy \"Vector databases\".\n",
    "The idea is to get a sense of the inner workings of a retrieval pipeline and understand the workflow from a user query to a generated response from an LLM.\n",
    "This end to end workflow will help you understand the importance of each step in a RAG pipeline.\n",
    "\n",
    "![](./imgs/SimpleRAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the data\n",
    "\n",
    "Each document contains a large number of tokens, so we need to split it into smaller chunks to manage the number of tokens per chunk. This approach serves three main purposes:\n",
    "\n",
    "* Most embedding models have a limit of tokens per input (based on their training data and parameters).\n",
    "\n",
    "* Chunking allows us to retrieve and send only the most relevant portions to our LLM, significantly reducing the total token count. This helps keep the LLM's cost and processing time manageable.\n",
    "\n",
    "* When the text is small-sized, embedding models tend to generate better vectors as they can capture more fine-grained details and nuances in the text, resulting in more accurate representations.\n",
    "\n",
    "![](./imgs/SimpleChunking.png)\n",
    "\n",
    "\n",
    "When choosing chunk size, consider these trade-offs:\n",
    "\n",
    "- Smaller chunks (100-200 tokens):\n",
    "  * More precise retrieval\n",
    "  * Better for finding specific details\n",
    "  * May lack broader context\n",
    "\n",
    "- Larger chunks (500-1000 tokens):\n",
    "  * Provide more context\n",
    "  * Capture more coherent ideas\n",
    "  * May introduce noise and reduce precision\n",
    "\n",
    "The optimal size depends on your data, expected queries, and model capabilities. Experiment with different sizes to find the best balance for your use case.\n",
    "\n",
    "Here we are chunking each content (text) to a maximum length of 500 tokens (`CHUNK_SIZE`). This is called **FIXED CHUNKING**. Although naive, it's a good starting point.\n",
    "\n",
    "For now, we will not be overlapping (`CHUNK_OVERLAP`) the content of one chunk with another chunk.\n",
    "\n",
    "We will be using the `chunk_simple` function to chunk the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = []\n",
    "for doc in raw_data.rows:\n",
    "    # Chunk the content of each document\n",
    "    chunks = chunk_simple(doc[\"content\"], chunk_size=500, model=\"gpt-4o-mini\")\n",
    "    # Create a unique ID for the document\n",
    "    doc_id = make_id(doc[\"content\"])\n",
    "    # Iterate over the chunks and create a new document for each chunk\n",
    "    for chunk in chunks:\n",
    "        doc_chunk = deepcopy(doc)\n",
    "        # Store the chunk as part of the document\n",
    "        doc_chunk[\"chunk\"] = chunk\n",
    "        # Convert the chunk to text to be used as input for the LLM\n",
    "        doc_chunk[\"text\"] = convert_contents_to_text(chunk)\n",
    "        # Create a unique ID for the chunk\n",
    "        doc_chunk[\"chunk_id\"] = make_id(chunk)\n",
    "        # Add the document ID to the chunk\n",
    "        doc_chunk[\"doc_id\"] = doc_id\n",
    "        # Append the chunk to the list of document chunks\n",
    "        document_chunks.append(doc_chunk)\n",
    "\n",
    "# Let's see the first 5 chunks\n",
    "document_chunks[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = weave.Dataset(name=\"chunked_data\", rows=document_chunks)\n",
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the simplest retrieval engine\n",
    "\n",
    "One of the key ingredient of most retrieval systems is to represent the given modality (text in our case) as a vector.\n",
    "\n",
    "This vector is a numerical representation representing the \"content\" of that modality (text).\n",
    "\n",
    "Text vectorization (text to vector) can be done using various techniques like \n",
    "- [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model), \n",
    "- [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (Term Frequency-Inverse Document Frequency), \n",
    "- and static embeddings like [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), [GloVe](https://nlp.stanford.edu/projects/glove/), \n",
    "- and context aware embeddings based on transformer based models like BERT and more, which capture the semantic meaning and relationships between words or sentences.\n",
    "\n",
    "\n",
    "First, we'll use TF-IDF (Term Frequency-Inverse Document Frequency) to vectorize our contents. Here's why:\n",
    "\n",
    "- **Simplicity**: TF-IDF is straightforward to implement and understand, making it an excellent starting point for RAG systems.\n",
    "- **Efficiency**: It's computationally lightweight, allowing for quick processing of large document collections.\n",
    "- **No training required**: Unlike embedding models, TF-IDF doesn't need pre-training, making it easy to get started quickly.\n",
    "- **Interpretability**: The resulting vectors are directly related to word frequencies, making them easy to interpret.\n",
    "\n",
    "While more advanced methods like embeddings **might** provide better performance, especially for semantic understanding, we'll explore these in later as we progress through the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How has Apple's total net sales changed over time?\"\n",
    "\n",
    "# our search engine a very simple class, that first indexes the documents and \n",
    "# then uses the search method to retrieve the top-k results \n",
    "# that are most similar to the query using cosine distance\n",
    "# these are what we call sparse retrieval engines because they are based on the term frequency of the words in the documents\n",
    "# they are more to do with lexical similarity than semantic similarity\n",
    "tfidf_search_engine = await TfidfSearchEngine().fit(document_chunks)\n",
    "\n",
    "# The retriever is a simple wrapper around the search engine that uses the search method\n",
    "# this is mostly for the sake of consistency and to have a common interface for all the retrieval engines\n",
    "class TFIDFRetriever(Retriever):\n",
    "    pass\n",
    "\n",
    "tfidf_retriever = TFIDFRetriever(search_engine=tfidf_search_engine)\n",
    "\n",
    "\n",
    "retrieved_docs = await tfidf_retriever.invoke(query=query, top_k=5)\n",
    "# we can look at the retrieved docs by printing them with the render_doc\n",
    "# basically a fancy print statement with some formatting for the doc content and metadata\n",
    "for doc in retrieved_docs:\n",
    "    render_doc(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `Retriever` class above is inherited from `weave.Model`.\n",
    "\n",
    "A `Model` is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates. By structuring your code to be compatible with this API, you benefit from a structured way to version your application so you can more systematically keep track of your experiments.\n",
    "\n",
    "To create a model in Weave, you need the following:\n",
    "\n",
    "- a class that inherits from `weave.Model`\n",
    "- type definitions on all attributes\n",
    "- a typed `predict`, `invoke` or `forward` method with `@weave.op` decorator.\n",
    "\n",
    "Imagine `weave.op` to be a drop in replacement for `print` or logging statement.\n",
    "However, it does a lot more than just printing and logging by tracking the inputs and outputs of the function and storing them as Weave objects.\n",
    "In addition to state tracking you also get a nice weave UI to inspect the inputs, outputs, and other metadata.\n",
    "\n",
    "If you have not initialized a weave run by doing `weave.init`, the code will work as it is without any tracking.\n",
    "\n",
    "The `predict` method decorated with `weave.op()` will track the model settings along with the inputs and outputs anytime you call it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a response\n",
    "\n",
    "There are two components of any RAG pipeline - a `Retriever` and a `ResponseGenerator`. Having designed a simple retriever, here we are designing a `SimpleResponseGenerator`.\n",
    "\n",
    "The `SimpleResponseGenerator` takes the user question along with the retrieved context (documents) as inputs and makes a LLM call using the `model` and `prompt` (system prompt). This way the generated answer is grounded on the documentation (our usecase). In this case we are using Cohere's `command-r` model.\n",
    "\n",
    "As earlier, we have wrapped this `SimpleResponseGenerator` class with weave for tracking the inputs and the output.\n",
    "\n",
    "The `ResponseGenerator` also has a `system_prompt` that we pass to the LLM. Consider this to be set of instructions we give to the LLM on what to do with the user's query and the retrieved documents.\n",
    "In practice, the system prompt can be detailed and involved (depending on the usecase) but we are using a very simple prompt here.\n",
    "Later we will iterate on it and show how improving the system prompt improves the quality of the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_data = [{\"document\": item[\"text\"]} for item in retrieved_docs]\n",
    "simple_response_generator = SimpleResponseGenerator()\n",
    "response = await simple_response_generator.invoke(\n",
    "    query=query, documents=docs_data\n",
    ")\n",
    "printmd(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing everything together\n",
    "\n",
    "Finally, with all the components ready, we can bring everything together into a single pipeline.\n",
    "\n",
    "Here, we define a `weave.Model` class `SimpleRAGPipeline` which combines the steps of retrieval and response generation.\n",
    "\n",
    "We'll define a `invoke` method that takes the user query, retrieves relevant context using the retriever and finally synthesizes a response using the response generator.\n",
    "\n",
    "We'll also define a few convinence methods to format the documents retrieved from the retriever and create a system prompt for the response generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using the SimpleRAGPipeline class to combine the steps of retrieval and response generation\n",
    "\n",
    "class TFIDFRAGPipeline(SimpleRAGPipeline):\n",
    "    pass\n",
    "\n",
    "# Instantiate the pipeline with the retriever and response generator\n",
    "tfidf_rag_pipeline = TFIDFRAGPipeline(\n",
    "    retriever=tfidf_retriever,\n",
    "    generator=simple_response_generator\n",
    ")\n",
    "\n",
    "# Invoke the pipeline with the user query\n",
    "response = await tfidf_rag_pipeline.invoke(query=query,)\n",
    "printmd(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the RAG Pipeline\n",
    "\n",
    "Before we starting making further changes to the RAG pipeline, it is a good practice to evaluate the current state of the pipeline\n",
    "This will help us understand the current performance of the pipeline and identify areas of improvement.\n",
    "Think of evaluation as a way to measure the performance of your system - a benchmark to compare the performance of your system against.\n",
    "\n",
    "Evaluating a RAG pipeline is a crucial step in ensuring its robustness and effectiveness.\n",
    "We will evaluate the two main components of a RAG pipeline - retriever and response generator.\n",
    "\n",
    "Evaluating the retriever can be considered component evaluation. \n",
    "Depending on your RAG pipeline, there can be a few components and for ensuring robustness of your system,\n",
    "it is recommended to come up with evaluation for each component.\n",
    "\n",
    "![](./imgs/EvolvingRAG.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data for evaluation\n",
    "\n",
    "We are using a subset of the evaluation dataset we had created for wandbot.\n",
    "\n",
    "Learn more about how we created the evaluation dataset here:\n",
    "\n",
    "- [How to Evaluate an LLM, Part 1: Building an Evaluation Dataset for our LLM System](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-Evaluate-an-LLM-Part-1-Building-an-Evaluation-Dataset-for-our-LLM-System--Vmlldzo1NTAwNTcy)\n",
    "- [How to Evaluate an LLM, Part 2: Manual Evaluation of Wandbot, our LLM-Powered Docs Assistant](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-Evaluate-an-LLM-Part-2-Manual-Evaluation-of-Wandbot-our-LLM-Powered-Docs-Assistant--Vmlldzo1NzU4NTM3)\n",
    "\n",
    "The main take away from these reports are:\n",
    "\n",
    "- we first deployed wandbot for internal usage based on rigorous eyeballing based evalution.\n",
    "- the user query distribution was throughly analyzed and clustered. we sampled a good representative queries from these clusters and created a gold standard set of queries.\n",
    "- we then used in-house MLEs to perform manual evaluation using Argilla. Creating such evaluation platforms are easy.\n",
    "- To summarize, speed is the key here. Use whatever means you have to create a meaningful eval set.\n",
    "\n",
    "The evaluation samples are logged as [`weave.Dataset`](https://wandb.github.io/weave/guides/core-types/datasets/). `weave.Dataset` enables you to collect examples for evaluation and automatically track versions for accurate comparisons.\n",
    "\n",
    "Below we will download the latest version locally with a simple API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are downloading the latest version of the dataset from Weave\n",
    "eval_dataset = weave.ref(\"weave:///a-sh0ts/rag-course-finance/object/eval_data:CoQDvdOENbZqkwg7IlhZm33drBCAf9OUNvf8ar6YHzM\").get()\n",
    "\n",
    "print(\"Number of evaluation samples: \", len(eval_dataset.rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through each sample is easy.\n",
    "\n",
    "We have the question, ground truth answer and ground truth contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use W&B Weave for our evaluation purposes.\n",
    "The `weave.Evaluation` class is a light weight class that can be used to evaluate the performance of a `weave.Model` on a `weave.Dataset`.\n",
    "\n",
    "### Evaluating the Retriever\n",
    "\n",
    "The fundamental idea of evaluating a retriever is to check how well the retrieved content matches the expected contents.\n",
    "To evaluate a RAG pipeline, we need query and ground truth answer pairs. \n",
    "The ground truth answer must be grounded on some \"ground\" truth chunks.\n",
    "This is a search problem, it's easiest to start with traditional Information retrieval metrics.\n",
    "\n",
    "\n",
    "\n",
    "You might already have access to such evaluation dataset depending on the nature of your application or you can synthetically build one. \n",
    "To build one you can retrieve random documents/chunks and ask an LLM to generate query-answer pairs - the underlying documents/chunks will act as your ground truth chunk.\n",
    "\n",
    "Here, we will look at different metrics that can be used to evaluate the retriever we built earlier.\n",
    "\n",
    "### Metrics to evaluate retriever\n",
    "\n",
    "We can evaluate a retriever using traditional ML metrics. We can also evaluate it by using a powerful LLM (next section).\n",
    "\n",
    "Below we are importing both traditional metrics and classifier based metric from the [`retrieval_metrics.py`](retrieval_metrics.py) file.\n",
    "\n",
    "* **Hit Rate**: Measures the proportion of queries where the retriever successfully returns at least one relevant document.\n",
    "* **MRR (Mean Reciprocal Rank)**: Evaluates how quickly the retriever returns the first relevant document, based on the reciprocal of its rank.\n",
    "* **NDCG (Normalized Discounted Cumulative Gain)**: Assesses the quality of the ranked retrieval results, giving more importance to relevant documents appearing earlier.\n",
    "* **MAP (Mean Average Precision)**: Computes the mean precision across all relevant documents retrieved, considering the rank of each relevant document.\n",
    "* **Precision**: Measures the ratio of relevant documents retrieved to the total documents retrieved by the retriever.\n",
    "* **Recall**: Evaluates the ratio of relevant documents retrieved to the total relevant documents available for the query.\n",
    "* **F1 Score**: The harmonic mean of precision and recall, providing a balance between both metrics to gauge retriever performance.\n",
    "* **Relevance Score**: A binary score predicted by a pre-trained classifier that measures the relevance of the retrieved documents to the query.\n",
    "\n",
    "We will be using the `RetrievalScorer` class to evaluate the retriever.\n",
    "Each metric expects an `output` which is a list of retrieved chunks from the retriever and `contexts` which is a list of ground truth contexts from the evaluation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=[RetrievalScorer(name=\"retrieval_scorer\", description=\"Retrieval metrics\")],\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"top_k\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_retrieval_scores = await retrieval_evaluation.evaluate(\n",
    "    model=tfidf_retriever,\n",
    "    __weave={\"display_name\": \"TFIDF Retrieval\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Response Generation\n",
    "\n",
    "Evaluating the generated response is a bit more complex. Although we have some ground truth answers, they are not directly comparable to the generated response. This could be due to a few reasons:\n",
    "\n",
    "- The generated response might be a paraphrased version of the ground truth answer.\n",
    "- The generated response might be a more detailed version of the ground truth answer.\n",
    "- The generated response might be a more concise version of the ground truth answer.\n",
    "- The generated response might be a more structured version of the ground truth answer.\n",
    "\n",
    "\n",
    "Some of these metrics are based on the distance between the generated response and the ground truth answer.\n",
    "- **Diff Score**: compute the similarity ratio between the normalized model output and the expected answer.\n",
    "- **Levenshtein Score**: compute the Levenshtein ratio between the normalized model output and the answer.\n",
    "- **ROUGE Score**: compute the ROUGE-L F1 score between the normalized model output and the reference answer.\n",
    "- **BLEU Score**: compute the BLEU score between the normalized model output and the reference answer.\n",
    "- **METEOR Score**: compute the METEOR score between the normalized model output and the reference answer.\n",
    "- **Correctness Score**: a binary score predicted by a pre-trained classifier that measures the correctness of the generated response.\n",
    "- **Helpfulness Score**: a binary score predicted by a pre-trained classifier that measures the helpfulness of the generated response.\n",
    "- **Relevance Score**: a binary score predicted by a pre-trained classifier that measures the relevance of the generated response to the query.\n",
    "\n",
    "While these metrics are not perfect, they provide a good north star for evaluating the response generation and can be used to compare the performance of the response generation model.\n",
    "\n",
    "We will be using the `ResponseScorer` class to evaluate the response generation.\n",
    "Each metric expects an `output` containing the generated response and the ground truth `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we are using the `weave.Evaluation` class to evaluate the response generation.\n",
    "response_evaluation = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=[ResponseScorer(name=\"response_scorer\", description=\"Response metrics\")],\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_response_scores = await response_evaluation.evaluate(\n",
    "    model=tfidf_rag_pipeline,\n",
    "    __weave={\"display_name\": \"TFIDF RAG Pipeline\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RAG Pipeline\n",
    "\n",
    "One of the most common ways to improve the performance of a RAG pipeline is to use a better retrieval algorithm. It is the low hanging fruit and often the first thing to try. This is because the retrieval step is one of the most important steps in a RAG pipeline. With a good retrieval algorithm, we can retrieve more relevant documents and improve the quality and relevance of the generated response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BM25 for Retrieval\n",
    "\n",
    "BM25 is a popular retrieval algorithm that uses the term frequency of the words in the documents to retrieve the most relevant documents. \n",
    "Many production IR systems use BM25.\n",
    "\n",
    "For instance Elasticsearch uses BM25 as its default retrieval algorithm. \n",
    "It's a simple algorithm that is easy to implement and has been shown to be effective in many cases.\n",
    "Although it doesn't perform semantic search, it's a good starting point for improving the retrieval step.\n",
    "You'll be suprised how much it alone can improve the performance of the RAG pipeline.\n",
    "\n",
    "We will be using the `BM25SearchEngine` class to create a BM25 search engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are still fitting the search engine on the same document chunks as we did for the TFIDF search engine\n",
    "bm25_search_engine = await BM25SearchEngine().fit(document_chunks)\n",
    "\n",
    "# A wrapper around the search engine that uses the search method\n",
    "class BM25Retriever(Retriever):\n",
    "    pass\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever(search_engine=bm25_search_engine)\n",
    "\n",
    "# retrieved_docs = await bm25_retriever.invoke(query=query, top_k=5)\n",
    "\n",
    "# for doc in retrieved_docs:\n",
    "#     render_doc(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now ready to evaluate the retrieval performance of the BM25 retrieval algorithm\n",
    "# we are using the same evaluation dataset as we did for the TFIDF retrieval algorithm\n",
    "# this should allow us to compare the performance of the two retrieval algorithms\n",
    "retrieval_scores = await retrieval_evaluation.evaluate(\n",
    "    model=bm25_retriever,\n",
    "    __weave={\"display_name\": \"BM25 Retrieval\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While evaluating the retrieval performance is important, we are interested more in the overall performance of the RAG pipeline.\n",
    "\n",
    "Again, we will be using the `SimpleRAGPipeline` class to create a RAG pipeline using the same response generator as we did earlier.\n",
    "\n",
    "We are also using the same evaluation dataset as we did for the TFIDF RAG pipeline. This should allow us to compare the performance of the two pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25RAGPipeline(SimpleRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "bm25_rag_pipeline = BM25RAGPipeline(\n",
    "    retriever=bm25_retriever,\n",
    "    generator=simple_response_generator\n",
    ")\n",
    "\n",
    "# response = await bm25_rag_pipeline.invoke(query=query)\n",
    "\n",
    "# printmd(response[\"answer\"])\n",
    "\n",
    "response_scores = await response_evaluation.evaluate(\n",
    "    model=bm25_rag_pipeline,\n",
    "    __weave={\"display_name\": \"BM25 RAG Pipeline\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dense Retrieval\n",
    "\n",
    "Dense retrieval is a retrieval algorithm that uses the embeddings of the documents to retrieve the most relevant documents.\n",
    "\n",
    "While technically we can use any machine learning model to create embeddings, we will use the embeddings created by some pre-trained embedding model here. Specifically, we will use the embeddings created by the `cohere` embed API.\n",
    "\n",
    "We model this using the `DenseSearchEngine` class to create a search engine that uses the embeddings to retrieve the most relevant documents using cosine distance.\n",
    "\n",
    "The retrieved documents are more likely to be semantically relevant than being lexically similar i.e. they are likely to be similar to the query in terms of the semantic meaning of the words rather than the exact words that are used in the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are fitting the search engine on the same document chunks as we did earlier\n",
    "dense_search_engine = DenseSearchEngine()\n",
    "dense_search_engine = await dense_search_engine.fit(document_chunks)\n",
    "\n",
    "# Another wrapper around the search engine\n",
    "class DenseRetriever(Retriever):\n",
    "    pass\n",
    "\n",
    "dense_retriever = DenseRetriever(search_engine=dense_search_engine)\n",
    "\n",
    "# retrieved_docs = await dense_retriever.invoke(query=query, top_k=5)\n",
    "# for doc in retrieved_docs:\n",
    "#     render_doc(doc)\n",
    "\n",
    "\n",
    "# # Let's evaluate the retrieval performance of the dense retrieval algorithm\n",
    "# retrieval_scores = await retrieval_evaluation.evaluate(\n",
    "#     model=dense_retriever,\n",
    "#     __weave={\"display_name\": \"Dense Retrieval\"}\n",
    "# )\n",
    "\n",
    "\n",
    "# The same drill, we create a RAG pipeline and evaluate it.\n",
    "class DenseRAGPipeline(SimpleRAGPipeline):\n",
    "    pass\n",
    "\n",
    "dense_rag_pipeline = DenseRAGPipeline(\n",
    "    retriever=dense_retriever,\n",
    "    generator=simple_response_generator\n",
    ")\n",
    "\n",
    "response_scores = await response_evaluation.evaluate(\n",
    "    model=dense_rag_pipeline,\n",
    "    __weave={\"display_name\": \"Dense RAG Pipeline\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking the contexts\n",
    "\n",
    "What if we can retrive documents ? i.e we cast a wider net and retrieve more documents to improve the recall of the retrieval step.\n",
    "\n",
    "However, with more contexts it is important to pick the ones that adds the most knowledge about the given query into the LLM context. i.e. we need to improve the precision of the retrieval step.\n",
    "\n",
    "![](./imgs/Reranking.png)\n",
    "\n",
    "For this, a re-ranking model (a separate machine learning model) can be used to calculate a matching score for a given query and document pair.\n",
    "This score can then be used to rearrange vector search results, ensuring that the most relevant results are prioritized at the top of the list.\n",
    "Cohere comes with it's own re-ranking model and is quite popular.\n",
    "\n",
    "\n",
    "The `DenseRerankedRetriever` class below uses a re-ranking model over dense search engine to retrieve the most relevant documents and then re-rank them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentially the search engine remains the same but the retrieved documents are re-ranked using a re-ranking model.\n",
    "\n",
    "class DenseRerankedRetriever(RetrieverWithReranker):\n",
    "    pass\n",
    "\n",
    "dense_reranked_retriever = DenseRerankedRetriever(search_engine=dense_search_engine,)\n",
    "\n",
    "\n",
    "# retrieved_docs = await dense_reranked_retriever.invoke(query=query, top_k=5)\n",
    "# for doc in retrieved_docs:\n",
    "#     render_doc(doc)\n",
    "\n",
    "## Evaluate and compare again with the reranked pipeline.\n",
    "\n",
    "# retrieval_scores = await retrieval_evaluation.evaluate(\n",
    "#     model=dense_reranked_retriever,\n",
    "#     __weave={\"display_name\": \"Dense Reranked Retrieval\"}\n",
    "# )\n",
    "\n",
    "class DenseRerankedRAGPipeline(SimpleRAGPipeline):\n",
    "    pass\n",
    "\n",
    "dense_reranked_rag_pipeline = DenseRerankedRAGPipeline(\n",
    "    retriever=dense_reranked_retriever,\n",
    "    generator=simple_response_generator\n",
    ")\n",
    "\n",
    "response_scores = await response_evaluation.evaluate(\n",
    "    model=dense_reranked_rag_pipeline,\n",
    "    __weave={\"display_name\": \"Dense Reranked RAG Pipeline\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Retrieveal\n",
    "\n",
    "Even though BM25 is an old model used for retrieval tasks, it is still the state-of-the-art on various benchmark. In machine learning, we ensemble a few weak classifiers to build a stronger classifier, we can adopt the same idea to our retriever pipeline.\n",
    "\n",
    "Here we show the concept of hybrid retriever which uses two or more retrievers and retrieves chunks from all of them followed by re-ranking.\n",
    "\n",
    "![](./imgs/HybridRetrieval.png)\n",
    "\n",
    "We use both the BM25 and Dense retrieval algorithms to retrieve chunks from both of them and then re-rank them.\n",
    "\n",
    "One additional thing to note here is to deduplicate the chunks retrieved from both the retrievers before re-ranking. This is because repeated chunks are not useful and can lead to a worse performance if not deduplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the hybrid retriever with the sparse and dense search engines.\n",
    "hybrid_retriever = HybridRetrieverWithReranker(\n",
    "    sparse_search_engine=bm25_search_engine,\n",
    "    dense_search_engine=dense_search_engine\n",
    ")\n",
    "\n",
    "# retrieved_docs = await hybrid_retriever.invoke(query=query, top_k=10, top_n=5)\n",
    "\n",
    "# for doc in retrieved_docs:\n",
    "#     render_doc(doc)\n",
    "\n",
    "\n",
    "# hybrid_retrieval_scores = await retrieval_evaluation.evaluate(\n",
    "#     model=hybrid_retriever,\n",
    "#     __weave={\"display_name\": \"Hybrid Retrieval\"}\n",
    "# )\n",
    "\n",
    "class HybridRAGPipeline(SimpleRAGPipeline):\n",
    "    pass\n",
    "\n",
    "hybrid_rag_pipeline = HybridRAGPipeline(\n",
    "    retriever=hybrid_retriever,\n",
    "    generator=simple_response_generator\n",
    ")\n",
    "\n",
    "hybrid_response_scores = await response_evaluation.evaluate(\n",
    "    model=hybrid_rag_pipeline,\n",
    "    __weave={\"display_name\": \"Hybrid RAG Pipeline\"}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we do better ?\n",
    "\n",
    "What if we have more data ? We initially used only the W&B documentation for our RAG pipeline. What if we use more data sources, like the `Weave Documentation`, `W&B Tutorials` and `Examples`, `W&B Blog posts`, `W&B Course content` etc. \n",
    "\n",
    "While it might improve the performance of the RAG pipeline, it might also increase the cost and complexity the pipeline. \n",
    "Our naive `DenseSearchEngine` might not be able to handle such a large corpus of documents because it requires storing the embeddings in memory.\n",
    "\n",
    "One specific way to handle some of these issues is to use a vector database to store the embeddings of the documents and then use a retriever to retrieve the most relevant documents.\n",
    "Vector databases are optimized for storing and querying embeddings and are a good way to handle large corpora of documents. \n",
    "This allows us to work with a large corpus of documents without dealing with the complexity of storing the embeddings in memory.\n",
    "\n",
    "We will be using the `VectorStoreSearchEngine` class to create a vector store search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset = load_dataset(docs_root)\n",
    "# chunked_dataset = chunk_dataset(full_dataset, chunk_size=500)\n",
    "# vectorstore_search_engine = VectorStoreSearchEngine()\n",
    "# vectorstore_search_engine = await vectorstore_search_engine.fit(chunked_dataset)\n",
    "vectorstore_search_engine = VectorStoreSearchEngine()\n",
    "vectorstore_search_engine = await vectorstore_search_engine.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additional advantage of using a vector database is we can run more complex queries that include metadata filters.\n",
    "\n",
    "For instance, we can retrieve all the markdown and notebook files from the dataset without retrieving the code documents. This can be useful if we are interested in retrieving a specific type of document or subset of documents before preforming the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = await vectorstore_search_engine.search(\n",
    "#     query=query,\n",
    "#     top_k=5,\n",
    "#     filters=\"file_type in ('notebook', 'markdown')\")\n",
    "# for doc in results:\n",
    "#     render_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreRetriever(RetrieverWithReranker):\n",
    "    pass\n",
    "\n",
    "vectorstore_retriever = VectorStoreRetriever(search_engine=vectorstore_search_engine)\n",
    "\n",
    "#TODO: Appropriate filters\n",
    "# results = await vectorstore_retriever.invoke(query=query, top_k=10, top_n=5, filters=\"file_type in ('notebook', 'markdown')\")\n",
    "# for doc in results:\n",
    "#     render_doc(doc)\n",
    "\n",
    "class VectorStoreRAGPipeline(SimpleRAGPipeline):\n",
    "    pass\n",
    "\n",
    "vectorstore_rag_pipeline = VectorStoreRAGPipeline(\n",
    "    retriever=vectorstore_retriever,\n",
    "    generator=simple_response_generator)\n",
    "\n",
    "# response = await vectorstore_rag_pipeline.invoke(query=query)\n",
    "\n",
    "# printmd(response[\"answer\"])\n",
    "\n",
    "# We can now evaluate the response generation performance of the vector store RAG pipeline.\n",
    "vectorstore_response_scores = await response_evaluation.evaluate(\n",
    "    model=vectorstore_rag_pipeline,\n",
    "    __weave={\"display_name\": \"Vector Store RAG Pipeline\"}\n",
    ")\n",
    "# Compare the scores of the vector store RAG pipeline with the earlier pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Enhancement\n",
    "\n",
    "Next let's look at another way to improve our RAG pipeline.\n",
    "We will be implementing a *query enhancement* stage in out pipeline.  Our `QueryEnhancer` will perform two key tasks:\n",
    "\n",
    "\n",
    "![QueryEnhancer](./imgs/Advanced_RAG_Pipeline.png)\n",
    "\n",
    "1. **Intent Classification**: Determine the user's intent of based on the query. This helps us better understand the user's query and improve the response generation step.\n",
    "\n",
    "2. **Query Expansion**: Break down queries into more focused sub-queries for retrieval. This improves retrieval by capturing different aspects of the original question.\n",
    "\n",
    "These enhancements serve two primary purposes:\n",
    "- Inform the response generator, allowing it to tailor its output based on language and intent.\n",
    "- Improve the retrieval process by using more targeted sub-queries.\n",
    "\n",
    "Let's implement our `QueryEnhancer` and see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again the `QueryEnhancer` is a `weave.Model`\n",
    "# it contains two methods, one to get the intent prediction and one to generate search queries.\n",
    "# the invoke method is a wrapper around the two methods.\n",
    "# let's see how it works for a sample query.\n",
    "query_enhancer = QueryEnhancer()\n",
    "results = await query_enhancer.invoke(query=query)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `QueryEnhancer` to create a `QueryEnhancedRAGPipeline`. However, unlike the `SimpleRAGPipeline`, this pipeline requires a `response_generator` that can handle the user's intent and query. Additionally, the `retriever` must now be able to handle the search queries generated by the `QueryEnhancer` and rerank and collate the results.\n",
    "\n",
    "The `QueryEnhancedRAGPipeline` encapsulates this logic and provides a way to unify the pipeline with the `QueryEnhancer` and the response generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the `QueryEnhancedRAGPipeline` with the `QueryEnhancer`, `retriever` and `QueryEnhancedResponseGenerator`.\n",
    "query_enhanced_rag_pipeline = QueryEnhancedRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=vectorstore_retriever,\n",
    "    generator=QueryEnhancedResponseGenerator()\n",
    ")\n",
    "# We can now evaluate the response generation performance of the query enhanced RAG pipeline.\n",
    "# response = await query_enhanced_rag_pipeline.invoke(query=query)\n",
    "# printmd(response[\"answer\"])\n",
    "\n",
    "query_enhanced_response_scores = await response_evaluation.evaluate(\n",
    "    model=query_enhanced_rag_pipeline,\n",
    "    __weave={\"display_name\": \"Query Enhanced RAG Pipeline\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic RAG\n",
    "\n",
    "So far we have seen a few ways to improve our RAG pipeline. \n",
    "The query enhanced RAG pipeline is already a little agentic in the sense that it uses the `QueryEnhancer` to generate search queries and then uses the `retriever` to retrieve the most relevant documents. The query enhancer uses a LLM to generate the search queries and intents predictions. We then manually route these to the retriever and response generator. It also made use of Structured Outputs to return the results in a structured format. - This was an example of an llm using tool calling. \n",
    "\n",
    "Tool use allows for greater flexibility in accessing and utilizing data sources, thus unlocking new use cases not possible with a standard RAG approach.\n",
    "\n",
    "In a setting where data sources are diverse with non-homogeneous formats (structured/semi-structured/unstructured), this approach becomes even more important.\n",
    "\n",
    "We'll look at how we can implement an agentic RAG system using a tool use approach. The agent can search for information about how to use the product, retrieve information from the internet, and search code examples\n",
    "\n",
    "Concretely, we'll cover the following use cases:\n",
    "\n",
    "- `search_docs`: Searches the filing documents in the SEC database\n",
    "- `search_internet`: Searches the internet for general queries\n",
    "\n",
    "With tool use we can take out pipeline a step further and create an agentic system.\n",
    "We will be using the `Agent` class to create an agentic RAG pipeline. \n",
    "\n",
    "The `Agent` class is a `weave.Model` that encapsulates some tools and llm calls and their routing logic. Let's see how it works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "\n",
    "answer = await agent.invoke(query=query)\n",
    "printmd(answer[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
